{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sQbtgMtVEJm1"
   },
   "source": [
    "### Instructions:\n",
    "-  Please add your IDs to the file name `Assignment1_notebook1_[student_ID]` (double click the name above to edit).<br>\n",
    "-  <b>Do not violate the [honor code](http://suraj.lums.edu.pk/~cs311w05/AcademicRules.htm#_edn1) of LUMS University.</b><br>\n",
    "-  Write clear and well commented code<br>\n",
    "-  A viva may also be conducted.<br>\n",
    "-  All solutions should be written in this notebook.\n",
    "-  The output of all cells should be present in the version of the notebook you submit.\n",
    "\n",
    "### This assigment will cover the following topics:\n",
    "\n",
    "<ol>\n",
    "<li>Regular Expressions(Regexps)</li>\n",
    "<li>Minimum Edit Distance</li>\n",
    "<li>NLTK Introduction</li>    \n",
    "<li>Application using TF-IDF(Part 2)</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rNdEWr7mEJm4"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IHDiQlWoEJm7"
   },
   "source": [
    "## Regular Expressions\n",
    "Regular expressions (regex) are really powerful. You certainly had fun in quiz 1 with regex. And Python does have a regex [library](https://docs.python.org/2/library/re.html) for you. Use them whenever you can. A beautiful line of regex can replace a chunk of ugly code. Once you've sufficient experience with regex, only then you'll really appreciate its utility. In this part of the notebook, you'll get some hands on experience on regular expressions.<br>\n",
    "\n",
    "Specifically, you will develop the following:\n",
    "<ul>\n",
    "<li>A Regular expression engine that checks for balanced parenthesis</li>\n",
    "<li>Links Detection inside HTML</li>\n",
    "<li>A Word Finder</li>\n",
    "</ul>\n",
    "You may find the following links helpfull:\n",
    "<ul>\n",
    "    <li><a href=\"https://docs.python.org/3/library/re.html\">Regex Documentation</a></li>\n",
    "    <li><a href=\"https://www.w3schools.com/python/python_regex.asp\">W3 School</a></li>\n",
    "    <li><a href=\"https://www.geeksforgeeks.org/pattern-matching-python-regex/\">GeeksforGeeks</a></li>\n",
    "    <li><a href=\"https://ibb.co/p4BFdvs\">Assignment Solution</a></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OuWC1YlKEJm8"
   },
   "source": [
    "### i. Regular expression engine that checks for balanced parenthesis\n",
    "Whenever a parenthesis is missing while you code in your favorite editor, odds are your editor will catch it. The editor will try to say parenthesis are unbalanced somewhere. Here, you'll use regular expressions to mimic that behavior of your editor. Specifically, you'll check whether the parenthesis in input text are balanced. <br>\n",
    "You will develop a regular expression engine that checks for balanced parenthesis in a given text/string. If the parenthesis are balanced and properly ordered (see examples), the function should return True. <b>Note that you may only use regular expressions along with a few `for` loops.</b> <br>\n",
    "Examples:\n",
    "<blockquote>\n",
    "12((23323))6----Balanced\n",
    "<br>44{21])954---Unblanced\n",
    "</blockquote>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A9vlZMMXEJm9"
   },
   "outputs": [],
   "source": [
    "def balanced_paranthesis_checker(string):\n",
    "    \"\"\" Checks if parenthesis are balanced and properly ordered.\n",
    "    args:\n",
    "        string: str -- raw text\n",
    "    Outputs:\n",
    "        is_balanced: Bool --  (True if parenthesis are balanced and properly ordered)\n",
    "    \"\"\"\n",
    "    is_balanced = False\n",
    "    ## ***** START OF YOUR CODE **** ##\n",
    "    \n",
    "    ## ***** END OF YOUR CODE ***** ##\n",
    "    return is_balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lBFMaWIUOhN1"
   },
   "source": [
    "Test your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qHa-93RzEJnA"
   },
   "outputs": [],
   "source": [
    "# ** Code Below is to test your implementation of balanced_paranthesis_checker (Do Not Modify). ** #\n",
    "tests=[('12{((22))}',True),('12{((22))',False),('[2123{(2(2)44)5}]',True),('12(22}',False),('12[(255)3)]}',False)]\n",
    "for string,truth in tests:\n",
    "    output=balanced_paranthesis_checker(string)\n",
    "    if(output!=None):\n",
    "        if(output==truth):\n",
    "            print(\"Test passed\",string,truth)\n",
    "        else:\n",
    "            print(\"Test Faild\",string,\"Expected:\",truth,\"Got:\",output)\n",
    "    else:\n",
    "        print(\"Check return type or solution is not implemented yet\")\n",
    "        break\n",
    "# ** Code Above is to test your implementation of balanced_paranthesis_checker (Do Not Modify). ** #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KLS-u695EJnE"
   },
   "source": [
    "### ii. Detect links inside HTML\n",
    "In many real world applications of Machine Learning, the very first stage (Data Extraction) may involve parsing raw HTML. You can use regex to extract useful information from HTML. Let's do so: <br>\n",
    "You are required to write a regexps to extract the links and the text name from the html of a page. \n",
    "<br>A html link is of the form:\n",
    "<br><blockquote>$\\text{<a href=\"https://lms.lums.edu.pk/portal\">LMS LUMS</a>}$</blockquote>\n",
    "<br>Here 'a' is the tag and href is an attribute which holds the link. The text name is LMS LUMS.\n",
    "<br><br>Text name can sometimes be hidden within multiple tags e.g\n",
    "<br><blockquote>$\\text{<a href=\"https://lms.lums.edu.pk/portal\"><h1><b>LMS LUMS</b></h1></a>}$</blockquote>\n",
    "<br>Here, the text name is hidden inside the tags 'h1' and 'b'.\n",
    "<br><br>Find all the links and the text name of the links in the following text.<br><blockquote>\n",
    "$\\text{<div class=\"portal\" role=\"navigation\" id='p-navigation'>}$<br>\n",
    "$\\text{<h3>Navigation</h3>}$<br>\n",
    "$\\text{<div class=\"body\">}$<br>\n",
    "$\\text{<ul>}$<br>\n",
    "$\\text{<li id=\"n-mainpage-description\"><a href=\"/wiki/Main_Page\" title=\"Visit the main page [z]\" accesskey=\"z\">Main page</a></li>}$<br>\n",
    "$\\text{<li id=\"n-contents\"><a href=\"/wiki/Portal:Contents\" title=\"Guides to browsing Wikipedia\">Contents</a></li>}$<br>\n",
    "$\\text{<li id=\"n-featuredcontent\"><a href=\"/wiki/Portal:Featured_content\" title=\"Featured content  the best of Wikipedia\">Featured content</a></li>}$<br>\n",
    "$\\text{<li id=\"n-currentevents\"><a href=\"/wiki/Portal:Current_events\" title=\"Find background information on current events\">Current events</a></li>}$<br>\n",
    "$\\text{<li id=\"n-randompage\"><a href=\"/wiki/Special:Random\" title=\"Load a random article [x]\" accesskey=\"x\">Random article</a></li>}$<br>\n",
    "$\\text{<li id=\"n-sitesupport\"><a href=\"//donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&utm_medium=sidebar&utm_campaign=C13_en.wikipedia.org&uselang=en\" title=\"Support us\">Donate to Wikipedia</a></li>}$<br>\n",
    "$\\text{</ul>}$<br>\n",
    "$\\text{</div>}$<br>\n",
    "$\\text{</div>}$<br>\n",
    "</blockquote>\n",
    "The output of your solution should return the following list of tuples:<br><blockquote>\n",
    "$\\text{/wiki/Main_Page,Main page}$<br>\n",
    "$\\text{/wiki/Portal:Contents,Contents}$<br>\n",
    "$\\text{/wiki/Portal:Featured_content,Featured content}$<br>\n",
    "$\\text{/wiki/Portal:Current_events,Current events}$<br>\n",
    "$\\text{/wiki/Special:Random,Random article}$<br>\n",
    "$\\text{//donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&utm_medium=sidebar&utm_campaign=C13_en.wikipedia.org&uselang=en,Donate to Wikipedia}$<br>\n",
    "</blockquote>\n",
    "Hint: You may need 2 regexps, one for link and one for text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2SPEvcLiEJnF"
   },
   "outputs": [],
   "source": [
    "def link_text_detector(html):\n",
    "    \"\"\" Checks if parenthesis are balanced and properly ordered.\n",
    "    args:\n",
    "        html: str -- contains the html text\n",
    "    Outputs:\n",
    "        link_text_detector_list: list -- list of tupples (link ,text)\n",
    "    \"\"\"\n",
    "    link_text_detector_list = []\n",
    "    ## ***** START OF YOUR CODE **** ##\n",
    "    \n",
    "    ## ***** END OF YOUR CODE ***** ##\n",
    "    return link_text_detector_list#returns tupple of url and text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "TRP6QAPVEJnI",
    "outputId": "188c9f20-9a3c-40c1-d70d-738019213701"
   },
   "outputs": [],
   "source": [
    "# ** Code Below is to test your implementation of link_text_detector (Do Not Modify). ** #\n",
    "text=\"\"\"<div class=\"portal\" role=\"navigation\" id='p-navigation'>\n",
    "<h3>Navigation</h3>\n",
    "<div class=\"body\">\n",
    "<ul>\n",
    "<li id=\"n-mainpage-description\"><a href=\"/wiki/Main_Page\" title=\"Visit the main page [z]\" accesskey=\"z\">Main page</a></li>\n",
    "<li id=\"n-contents\"><a href=\"/wiki/Portal:Contents\" title=\"Guides to browsing Wikipedia\">Contents</a></li>\n",
    "<li id=\"n-featuredcontent\"><a href=\"/wiki/Portal:Featured_content\" title=\"Featured content the best of Wikipedia\">Featured content</a></li>\n",
    "<li id=\"n-currentevents\"><a href=\"/wiki/Portal:Current_events\" title=\"Find background information on current events\">Current events</a></li>\n",
    "<li id=\"n-randompage\"><a href=\"/wiki/Special:Random\" title=\"Load a random article [x]\" accesskey=\"x\">Random article</a></li>\n",
    "<li id=\"n-sitesupport\"><a href=\"//donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&utm_medium=sidebar&utm_campaign=C13_en.wikipedia.org&uselang=en\" title=\"Support us\">Donate to Wikipedia</a></li>\n",
    "</ul>\n",
    "</div>\n",
    "</div>\"\"\"\n",
    "returned_tupples=link_text_detector(text)\n",
    "\n",
    "check_tupples=[(\"/wiki/Main_Page\",\"Main page\")\n",
    ",(\"/wiki/Portal:Contents\",\"Contents\")\n",
    ",(\"/wiki/Portal:Featured_content\",\"Featured content\")\n",
    ",(\"/wiki/Portal:Current_events\",\"Current events\")\n",
    ",(\"/wiki/Special:Random\",\"Random article\")\n",
    ",(\"//donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&utm_medium=sidebar&utm_campaign=C13_en.wikipedia.org&uselang=en\",\"Donate to Wikipedia\")]\n",
    "try:\n",
    "    if(len(returned_tupples)!=len(check_tupples)):\n",
    "        print(\"Error: Length of tupple lists not equal\")\n",
    "    else:\n",
    "        for p1, p2 in zip(check_tupples, returned_tupples):\n",
    "            if(p1[0]==p2[0] and p1[1]==p2[1]):\n",
    "                print(\"Check Passed\",p1[0],p1[1])\n",
    "            else:\n",
    "                print(\"Test Failed\\nRequired: \",p1[0],p1[1],\"\\nGot:\",p2[0],p2[1])\n",
    "                break\n",
    "except:\n",
    "    print(\"Exception occured\")\n",
    "# ** Code Above is to test your implementation of link_text_detector (Do Not Modify). ** #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nXzr03SuEJnM"
   },
   "source": [
    "### iii. Word Finder\n",
    "Given some sentences and words, for each of these words you are to find the count of its occurences in all the sentences.<b>We define a word as a non-empty maximum sequence of characters that can contain only lowercase letters, uppercase letters, digits and underscores '_'.</b><br>\n",
    "Example:\n",
    "<blockquote>rare21 pepe (pepe) rare rare-pepe rare_pepe rare'pepe pepe123 pepe,rare.</blockquote>\n",
    "Occurance count for word <b>pepe</b>:\n",
    "<blockquote>5</blockquote>\n",
    "Explaination for occuranc count:\n",
    "<ul>\n",
    "<li>pepe is a single word(Counted)</li>\n",
    "<li>(pepe) is preceeded by '(' and followed by ')', so it's the second word(Counted)</li>\n",
    "<li>rare-pepe is considered as two words and 'pepe' is the second word(Counted)</li>\n",
    "<li>rare_pepe is a single single word(NotCounted)</li>\n",
    "<li>pepe123 is a single single word(NotCounted)</li>\n",
    "<li>rare'pepe is considered as two words and 'pepe' is the second word(Counted)</li>\n",
    "<li>pepe, as it is preceeded by a space and followed by a dot ','(Counted)</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cEQ1lvjUEJnN"
   },
   "outputs": [],
   "source": [
    "def word_finder(sentences,words):\n",
    "    \"\"\" Checks if parenthesis are balanced and properly ordered.\n",
    "    args:\n",
    "        sentences: list -- list of sentences\n",
    "        words: list -- list of words\n",
    "    Outputs:\n",
    "        word_finder_list: list -- list of tupples (word,count)\n",
    "    \"\"\"\n",
    "    word_finder_list = []\n",
    "    ## ***** START OF YOUR CODE **** ##\n",
    "    \n",
    "    ## ***** END OF YOUR CODE ***** ##\n",
    "    return word_finder_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GsDOsc5uEJnQ"
   },
   "outputs": [],
   "source": [
    "# ** Code Below is to test your implementation of word_finder (Do Not Modify). ** #\n",
    "sentences=[\"rare21 pepe (rare) pepe rare-pepe rare_pepe foo'pepe pepe123 pepe,rare.\"\n",
    "        \"foo bar (foo) bar foo-bar foo_bar foo'bar bar-foo bar, foo.\",\n",
    "        \"bamboozled bamboozled-doggo bamboozled_doggo doggo bamboozled bamboozled2 bamboozled*doggo\",]\n",
    "words=[\"pepe\",\"foo\",\"bamboozled\"]\n",
    "counts=[(\"pepe\",5),(\"foo\",6),(\"bamboozled\",4)]\n",
    "output=word_finder(sentences,words)\n",
    "try:\n",
    "    print(len(counts),len(output))\n",
    "    if(len(counts)!=len(output)):\n",
    "        print(\"Error: Length of tupple lists not equal\")\n",
    "    else:\n",
    "        for p1, p2 in zip(counts, output):\n",
    "            if(p1[0]==p2[0] and p1[1]==p2[1]):\n",
    "                print(\"Check Passed\",p1[0],p1[1])\n",
    "            else:\n",
    "                print(\"Test Failed\\nRequired: \",p1[0],p1[1],\"\\nGot:\",p2[0],p2[1])\n",
    "                break\n",
    "except:\n",
    "    print(\"Exception occured\")\n",
    "# ** Code Above is to test your implementation of word_finder (Do Not Modify). ** #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1Rb4QpXTEJnT"
   },
   "source": [
    "## 2) Minimum Edit Distance\n",
    "You are to design a general purpose solution for minimum edit distance that works for different values of insertion, deletion, subsitution. <a href=\"https://web.stanford.edu/class/cs124/lec/med.pdf\">Slides</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oOEmE4VeEJnU"
   },
   "outputs": [],
   "source": [
    "def minimum_edit_distance(source,target,cost_insertion=1,cost_deletion=1,cost_subsitution=1):\n",
    "    \"\"\" Implement the minimum edit distance algorithm.\n",
    "    args:\n",
    "        source: str -- source string\n",
    "        target: str -- target string\n",
    "        cost_insertion: int -- cost of insertion\n",
    "        cost_subsitution: int -- cost of subsitution\n",
    "        cost_deletion: int -- cost of deletion\n",
    "    Outputs:\n",
    "        min_edit_dist: int -- minimum edit distance\n",
    "    \"\"\"\n",
    "    min_edit_dist = 0\n",
    "    ## ***** START OF YOUR CODE **** ##\n",
    "    \n",
    "    ## ***** END OF YOUR CODE ***** ##\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CGWu2JqMEJnX"
   },
   "outputs": [],
   "source": [
    "# ** Code Below is to test your implementation of minimum_edit_distance (Do Not Modify). ** #\n",
    "test_cases=[(\"intention\",\"execution\",1,1,2,8),(\"intention\",\"execution\",1,1,1,5),(\"trails\",\"ziel\",1,1,2,6),(\"intention\",\"execution\",2,2,2,9),(\"trails\",\"ziel\",1,2,2,7),(\"trails\",\"ziel\",2,1,1,5)]\n",
    "for case in test_cases:\n",
    "    output=minimum_edit_distance(case[0],case[1],case[2],case[3],case[4])\n",
    "    if(case[5]==output):\n",
    "        continue\n",
    "    else:\n",
    "        print(\"failed, for source:\",case[0],\" and target:\",case[1],\"\\nintertion,deletion and subsitution cost:\",case[2],case[3],case[4],\"\\nExpected output\",case[5],\" Got:\",output)\n",
    "        break\n",
    "# ** Code Above is to test your implementation of minimum_edit_distance (Do Not Modify). ** #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wd-JmKN0EJnZ"
   },
   "source": [
    "## 3) NLTK\n",
    "In this part of the assignment you will be introduced to natural language toolkit or nltk. NLTK is the swiss army knife of natural language processing it contains many corporas, basic machine learning algorithm and text preprocessing techniques.  But it is primarily used for preprocessing step in the machine learning pipeline.\n",
    "This part of the assignment is meant to be an introduction to the following form nltk:\n",
    "<ul>\n",
    "<li>Tokenization</li>\n",
    "<li>Stopwords removal</li>\n",
    "<li>Stemming</li>\n",
    "<li>Lemmatization</li>\n",
    "<li>Frequence Distribution</li>\n",
    "</ul>\n",
    "First run the command in the cell below, it should open a GUI from where you can download different packages, collections ,models and corporas. For now, install the book collection. You can also use the command <b>nltk.download('package_name')</b> instead of using the GUI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rIGwCxjSEJna"
   },
   "outputs": [],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hqSTHiAlEJnc"
   },
   "outputs": [],
   "source": [
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qb9vaGg_EJnf"
   },
   "source": [
    "The texts are stored in the varables text1,text2 and so on, we will be using text 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GeBX6IMUEJng"
   },
   "outputs": [],
   "source": [
    "text3.generate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kSVUcr5ZEJni"
   },
   "source": [
    "Although the code below is redundant, we want to to use <b>tokenizer from nltk to tokenize the data</b>. The data by default is in tokenized form as demonstrated below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HdTWCLEKEJnj"
   },
   "outputs": [],
   "source": [
    "corpora=\" \".join(text3)#joins the token based on space delimiter\n",
    "print(text3[:15])#prints the first fifteen tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2TwhBKdQEJnm"
   },
   "source": [
    "For all of tasks below consult the documentation <a href=\"https://www.nltk.org/book/ch01.html\">here</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7iq3Kn3vEJnn"
   },
   "source": [
    "### a)Tokenization\n",
    "Tokenize text3 and print the first 20 tokens. For those of you who are new to python its a good time to learn about <a href=\"https://railsware.com/blog/python-for-machine-learning-indexing-and-slicing-for-lists-tuples-strings-and-other-sequential-types/\">array slices</a>.\n",
    "<br><a href=\"https://www.nltk.org/api/nltk.tokenize.html\">Tokenize Documentation</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X4bmlsT8EJnp"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yrd4RMuoEJnr"
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\" Tokenize the text using word_tokenize\n",
    "    args:\n",
    "        text: str -- corpus\n",
    "    Outputs:\n",
    "        tokens: list of word tokens\n",
    "    \"\"\"\n",
    "    tokens=[]\n",
    "    ## ***** START OF YOUR CODE **** ##\n",
    "    \n",
    "    ## ***** END OF YOUR CODE ***** ##\n",
    "    return tokens\n",
    "    \n",
    "tokenized=tokenize(corpora)\n",
    "#add code here to print first 20 tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vBL5HG2eEJnt"
   },
   "source": [
    "### b)Total count of  tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RlvQToDOEJnu"
   },
   "outputs": [],
   "source": [
    "def token_count(tokens):\n",
    "    \"\"\" Total count of tokens\n",
    "    args:\n",
    "        tokens: list of word tokens\n",
    "    Outputs:\n",
    "        count : int -- number of total tokens (integer)\n",
    "    \"\"\"\n",
    "    count=0\n",
    "    ## ***** START OF YOUR CODE **** ##\n",
    "    \n",
    "    ## ***** END OF YOUR CODE ***** ##\n",
    "    return count #return token count\n",
    "\n",
    "total_tokens=token_count(tokenized)\n",
    "print(\"Total token count\",total_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Az5LQ4sGEJnw"
   },
   "source": [
    "### c)Total count of unique tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6k9tTrkgEJny"
   },
   "outputs": [],
   "source": [
    "def token_count_unique(tokens):\n",
    "    \"\"\" Total count of unique tokens\n",
    "    args:\n",
    "        tokens: list of word tokens\n",
    "    Outputs:\n",
    "        count : int -- number of total unique tokens (integer)\n",
    "    \"\"\"\n",
    "    unique_count=0\n",
    "    ## ***** START OF YOUR CODE **** ##\n",
    "    \n",
    "    ## ***** END OF YOUR CODE ***** ##\n",
    "    return unique_count#return unique token count\n",
    "total_tokens_unique=token_count_unique(tokenized)\n",
    "print(\"Total unique token count\",total_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QCXGPEnjEJn0"
   },
   "source": [
    "### d)Vocabulary\n",
    "Print the entire vocabilary, here you should keep the definition of vocabulary in mind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YvN23xOdEJn1"
   },
   "outputs": [],
   "source": [
    "def vocabulary(tokens):\n",
    "    \"\"\" Print entire vocabulary\n",
    "    args:\n",
    "        tokens: list of word tokens\n",
    "    Outputs:\n",
    "        list : vocabulary(tokens)\n",
    "    \"\"\"\n",
    "    ## ***** START OF YOUR CODE **** ##\n",
    "    \n",
    "    ## ***** END OF YOUR CODE ***** ##\n",
    "    return None #returns a list of tokens\n",
    "print(vocabulary(tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7BpS0qBDEJn4"
   },
   "source": [
    "### e)Frequency Distribution\n",
    "Print the Frequency Distribution for the top 50 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c7ZM49LgEJn4"
   },
   "outputs": [],
   "source": [
    "from nltk import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_yrwM9UoEJn6"
   },
   "outputs": [],
   "source": [
    "def frequency_distribution(tokens):\n",
    "    \"\"\" Print entire vocabulary\n",
    "    args:\n",
    "        tokens: list of word tokens\n",
    "    Outputs:\n",
    "         frequency_distribution: FreqDist -- frequency disribution\n",
    "    \"\"\"\n",
    "    ## ***** START OF YOUR CODE **** ##\n",
    "    \n",
    "    ## ***** END OF YOUR CODE ***** ##\n",
    "    return _frequency_distribution\n",
    "    \n",
    "freqdist=frequency_distribution(tokenized)\n",
    "#add code here to print top 50 words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z7cm-qsZEJoC"
   },
   "source": [
    "### f) Bigram Frequence Distribution\n",
    "Print the Bigram Frequence Distribution for top 50 bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bUOWw9_vEJoD"
   },
   "outputs": [],
   "source": [
    "def frequency_distribution_bigram(text):\n",
    "    \"\"\" Print entire vocabulary\n",
    "    args:\n",
    "        tokens: list of word tokens\n",
    "    Outputs:\n",
    "        frequency_distribution_bigram: FreqDist -- frequency distribution of bigrams\n",
    "    \"\"\"\n",
    "    ## ***** START OF YOUR CODE **** ##\n",
    "    \n",
    "    ## ***** END OF YOUR CODE ***** ##\n",
    "    return frequency_distribution_bigram#returns a Frequency Distribution of bigrams\n",
    "\n",
    "freqdist_bigram=frequency_distribution_bigram(tokenized)\n",
    "#add code here to print top 50 bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y-rBpzlnEJoG"
   },
   "source": [
    "### f)Compute lexical diversity\n",
    "Here we will define lexical diversity as the ratio between the number of unique tokens and total number of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X-o2ysGyEJoG"
   },
   "outputs": [],
   "source": [
    "def lexical_diversity(tokens):\n",
    "    \"\"\" Print entire vocabulary\n",
    "    args:\n",
    "        tokens: list of word tokens\n",
    "    Outputs:\n",
    "        diversity_score: float -- lexical diversity \n",
    "    \"\"\"\n",
    "    ## ***** START OF YOUR CODE **** ##\n",
    "    \n",
    "    ## ***** END OF YOUR CODE ***** ##\n",
    "    return diversity_score\n",
    "\n",
    "lexical_diversity_score=lexical_diversity(tokenized)\n",
    "print(\"Lexical diversity score\",lexical_diversity_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wTNzsicUEJoJ"
   },
   "source": [
    "### g)Remove Stopwords\n",
    "Read the documentation and remove the <a href=\"https://en.wikipedia.org/wiki/Stop_words\">stopwords</a> in `tokenized` list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z4Sb3ktWEJoJ"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wf14EDbNEJoM"
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(tokens):\n",
    "    \"\"\" Remove stopwords\n",
    "    args:\n",
    "        tokens: list -- list of word tokens\n",
    "    Outputs:\n",
    "        stopwords_removed_tokens: list -- list of tokens with stopwords removed\n",
    "    \"\"\"\n",
    "    ## ***** START OF YOUR CODE **** ##\n",
    "    \n",
    "    ## ***** END OF YOUR CODE ***** ##\n",
    "    return stopwords_removed_tokens\n",
    "tokenized_stopword=remove_stopwords(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2_SzKfbGr63_"
   },
   "outputs": [],
   "source": [
    "print(tokenized_stopword)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KgKuGmDpEJoO"
   },
   "source": [
    "### h)Lemmatization\n",
    "Apply <a href=\"https://en.wikipedia.org/wiki/Lemmatisation\">Lemmatization</a> the words in `tokenized` list and see its effects on the token list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eevtSKUhEJoP"
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W2ep0yIwEJoS"
   },
   "outputs": [],
   "source": [
    "def lemmatization(tokens):\n",
    "    \"\"\" Apply lemamatization\n",
    "    args:\n",
    "        tokens: list -- list of word tokens\n",
    "    Outputs:\n",
    "        lemmatized_tokens: list -- list of lemmatized word tokens\n",
    "    \"\"\"\n",
    "    ## ***** START OF YOUR CODE **** ##\n",
    "    \n",
    "    ## ***** END OF YOUR CODE ***** ##\n",
    "    return lemmatized_tokens\n",
    "lemmatized_tokens=lemmatization(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LaYsy1Pkr64H"
   },
   "outputs": [],
   "source": [
    "print(lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n9cjn2TZEJoW"
   },
   "source": [
    "### i)Stemming\n",
    "Apply <a href=\"https://en.wikipedia.org/wiki/Stemming\">Stemming</a> to the words in `tokenized` list list and see its effects on the token list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KbeFobzdEJoW"
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fUX2NsQcEJoY"
   },
   "outputs": [],
   "source": [
    "def stemming(tokens):\n",
    "    \"\"\" Apply Stemming\n",
    "    args:\n",
    "        tokens: list -- list of word tokens\n",
    "    Outputs:\n",
    "        stemmed_tokens: list -- list of stemmed tokens \n",
    "    \"\"\"\n",
    "    ## ***** START OF YOUR CODE **** ##\n",
    "    \n",
    "    ## ***** END OF YOUR CODE ***** ##\n",
    "    return stemmed_tokens\n",
    "stemmed_tokens=stemming(tokenized)\n",
    "tokenized_preprocessed=stemmed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MqVZ92Lar64P"
   },
   "outputs": [],
   "source": [
    "print(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gUL6WBWTEJoa"
   },
   "source": [
    "### j)Recompute\n",
    "Now repeat parts b through e for the new `lemmatized_tokens` list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IUK6E7mYEJob"
   },
   "outputs": [],
   "source": [
    "#part b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "58U8a6WYEJod"
   },
   "outputs": [],
   "source": [
    "#part c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rF53voG0EJog"
   },
   "outputs": [],
   "source": [
    "#part d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xcIpa85kEJoj"
   },
   "outputs": [],
   "source": [
    "#part e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rMaOTfaWEJol"
   },
   "source": [
    "## k) Lemmatization vs Stemming\n",
    "You have learnt about [stemming](https://en.wikipedia.org/wiki/stemming) and [lemmatization](https://en.wikipedia.org/wiki/Lemmatisation) in class, usually one of them is used according to requirements of the task you are trying to solve. Discuss in what situations might one be preferable over the other? <a href=\"https://ibb.co/p3f4Wtd\">In other words</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N_Nnrhq-EJom"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## ***** START OF YOUR ANSWER **** ##\n",
    "\n",
    "\n",
    "\n",
    "## ***** END OF YOUR ANSWER **** ##\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0FDy16bcr64i"
   },
   "source": [
    "Great you're done with part 1; Now, move onto <b><a href=\"https://ibb.co/MfpxHZh\"> part 2</a></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oJP5-RlOr64i"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "7iq3Kn3vEJnn",
    "vBL5HG2eEJnt",
    "Az5LQ4sGEJnw",
    "QCXGPEnjEJn0",
    "7BpS0qBDEJn4",
    "Z7cm-qsZEJoC",
    "Y-rBpzlnEJoG",
    "wTNzsicUEJoJ",
    "KgKuGmDpEJoO",
    "n9cjn2TZEJoW",
    "gUL6WBWTEJoa"
   ],
   "name": "Assignment1_notebook1_[student_ID].ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
