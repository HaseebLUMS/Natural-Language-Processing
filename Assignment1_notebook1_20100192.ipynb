{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sQbtgMtVEJm1"
   },
   "source": [
    "### Instructions:\n",
    "-  Please add your IDs to the file name `Assignment1_notebook1_[student_ID]` (double click the name above to edit).<br>\n",
    "-  <b>Do not violate the [honor code](http://suraj.lums.edu.pk/~cs311w05/AcademicRules.htm#_edn1) of LUMS University.</b><br>\n",
    "-  Write clear and well commented code<br>\n",
    "-  A viva may also be conducted.<br>\n",
    "-  All solutions should be written in this notebook.\n",
    "-  The output of all cells should be present in the version of the notebook you submit.\n",
    "\n",
    "### This assigment will cover the following topics:\n",
    "\n",
    "<ol>\n",
    "<li>Regular Expressions(Regexps)</li>\n",
    "<li>Minimum Edit Distance</li>\n",
    "<li>NLTK Introduction</li>    \n",
    "<li>Application using TF-IDF(Part 2)</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rNdEWr7mEJm4"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IHDiQlWoEJm7"
   },
   "source": [
    "## Regular Expressions\n",
    "Regular expressions (regex) are really powerful. You certainly had fun in quiz 1 with regex. And Python does have a regex [library](https://docs.python.org/2/library/re.html) for you. Use them whenever you can. A beautiful line of regex can replace a chunk of ugly code. Once you've sufficient experience with regex, only then you'll really appreciate its utility. In this part of the notebook, you'll get some hands on experience on regular expressions.<br>\n",
    "\n",
    "Specifically, you will develop the following:\n",
    "<ul>\n",
    "<li>A Regular expression engine that checks for balanced parenthesis</li>\n",
    "<li>Links Detection inside HTML</li>\n",
    "<li>A Word Finder</li>\n",
    "</ul>\n",
    "You may find the following links helpfull:\n",
    "<ul>\n",
    "    <li><a href=\"https://docs.python.org/3/library/re.html\">Regex Documentation</a></li>\n",
    "    <li><a href=\"https://www.w3schools.com/python/python_regex.asp\">W3 School</a></li>\n",
    "    <li><a href=\"https://www.geeksforgeeks.org/pattern-matching-python-regex/\">GeeksforGeeks</a></li>\n",
    "    <li><a href=\"https://ibb.co/p4BFdvs\">Assignment Solution</a></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OuWC1YlKEJm8"
   },
   "source": [
    "### i. Regular expression engine that checks for balanced parenthesis\n",
    "Whenever a parenthesis is missing while you code in your favorite editor, odds are your editor will catch it. The editor will try to say parenthesis are unbalanced somewhere. Here, you'll use regular expressions to mimic that behavior of your editor. Specifically, you'll check whether the parenthesis in input text are balanced. <br>\n",
    "You will develop a regular expression engine that checks for balanced parenthesis in a given text/string. If the parenthesis are balanced and properly ordered (see examples), the function should return True. <b>Note that you may only use regular expressions along with a few `for` loops.</b> <br>\n",
    "Examples:\n",
    "<blockquote>\n",
    "12((23323))6----Balanced\n",
    "<br>44{21])954---Unblanced\n",
    "</blockquote>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A9vlZMMXEJm9"
   },
   "outputs": [],
   "source": [
    "def balanced_paranthesis_checker(string):\n",
    "    \"\"\" Checks if parenthesis are balanced and properly ordered.\n",
    "    args:\n",
    "        string: str -- raw text\n",
    "    Outputs:\n",
    "        is_balanced: Bool --  (True if parenthesis are balanced and properly ordered)\n",
    "    \"\"\"\n",
    "    is_balanced = False\n",
    "    ## ***** START OF YOUR CODE **** ##\n",
    "    pattern = string\n",
    "    small_brack = \"\\([^\\{\\}\\[\\]\\(\\)]*\\)\"\n",
    "    curly_brack = \"\\{[^\\{\\}\\[\\]\\(\\)]*\\}\"\n",
    "    squar_brack = \"\\[[^\\{\\}\\[\\]\\(\\)]*\\]\"\n",
    "    regexes = [small_brack, curly_brack, squar_brack]\n",
    "    consececutive_four = 0\n",
    "    i = 0\n",
    "    while True:\n",
    "        if consececutive_four >= 4:break\n",
    "        i = (i + 1)%len(regexes)\n",
    "        r = re.compile(regexes[i])\n",
    "        while True:\n",
    "            dump = r.findall(pattern)\n",
    "            consececutive_four += 1\n",
    "            if len(dump) < 1: break\n",
    "            for d in dump:\n",
    "                pattern = pattern.replace(d, \"\")\n",
    "            consececutive_four = 0\n",
    "    if (\"(\" in pattern) or (\")\" in pattern) or (\"[\" in pattern) or (\"]\" in pattern) or (\"{\" in pattern) or (\"}\" in pattern):\n",
    "        return False\n",
    "    return True\n",
    "    ## ***** END OF YOUR CODE ***** ##\n",
    "#     return is_balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lBFMaWIUOhN1"
   },
   "source": [
    "Test your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qHa-93RzEJnA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed 12{((22))} True\n",
      "Test passed 12{((22)) False\n",
      "Test passed [2123{(2(2)44)5}] True\n",
      "Test passed 12(22} False\n",
      "Test passed 12[(255)3)]} False\n"
     ]
    }
   ],
   "source": [
    "# ** Code Below is to test your implementation of balanced_paranthesis_checker (Do Not Modify). ** #\n",
    "tests=[('12{((22))}',True),('12{((22))',False),('[2123{(2(2)44)5}]',True),('12(22}',False),('12[(255)3)]}',False)]\n",
    "for string,truth in tests:\n",
    "    output=balanced_paranthesis_checker(string)\n",
    "    if(output!=None):\n",
    "        if(output==truth):\n",
    "            print(\"Test passed\",string,truth)\n",
    "        else:\n",
    "            print(\"Test Faild\",string,\"Expected:\",truth,\"Got:\",output)\n",
    "    else:\n",
    "        print(\"Check return type or solution is not implemented yet\")\n",
    "        break\n",
    "# ** Code Above is to test your implementation of balanced_paranthesis_checker (Do Not Modify). ** #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KLS-u695EJnE"
   },
   "source": [
    "### ii. Detect links inside HTML\n",
    "In many real world applications of Machine Learning, the very first stage (Data Extraction) may involve parsing raw HTML. You can use regex to extract useful information from HTML. Let's do so: <br>\n",
    "You are required to write a regexps to extract the links and the text name from the html of a page. \n",
    "<br>A html link is of the form:\n",
    "<br><blockquote>$\\text{<a href=\"https://lms.lums.edu.pk/portal\">LMS LUMS</a>}$</blockquote>\n",
    "<br>Here 'a' is the tag and href is an attribute which holds the link. The text name is LMS LUMS.\n",
    "<br><br>Text name can sometimes be hidden within multiple tags e.g\n",
    "<br><blockquote>$\\text{<a href=\"https://lms.lums.edu.pk/portal\"><h1><b>LMS LUMS</b></h1></a>}$</blockquote>\n",
    "<br>Here, the text name is hidden inside the tags 'h1' and 'b'.\n",
    "<br><br>Find all the links and the text name of the links in the following text.<br><blockquote>\n",
    "$\\text{<div class=\"portal\" role=\"navigation\" id='p-navigation'>}$<br>\n",
    "$\\text{<h3>Navigation</h3>}$<br>\n",
    "$\\text{<div class=\"body\">}$<br>\n",
    "$\\text{<ul>}$<br>\n",
    "$\\text{<li id=\"n-mainpage-description\"><a href=\"/wiki/Main_Page\" title=\"Visit the main page [z]\" accesskey=\"z\">Main page</a></li>}$<br>\n",
    "$\\text{<li id=\"n-contents\"><a href=\"/wiki/Portal:Contents\" title=\"Guides to browsing Wikipedia\">Contents</a></li>}$<br>\n",
    "$\\text{<li id=\"n-featuredcontent\"><a href=\"/wiki/Portal:Featured_content\" title=\"Featured content  the best of Wikipedia\">Featured content</a></li>}$<br>\n",
    "$\\text{<li id=\"n-currentevents\"><a href=\"/wiki/Portal:Current_events\" title=\"Find background information on current events\">Current events</a></li>}$<br>\n",
    "$\\text{<li id=\"n-randompage\"><a href=\"/wiki/Special:Random\" title=\"Load a random article [x]\" accesskey=\"x\">Random article</a></li>}$<br>\n",
    "$\\text{<li id=\"n-sitesupport\"><a href=\"//donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&utm_medium=sidebar&utm_campaign=C13_en.wikipedia.org&uselang=en\" title=\"Support us\">Donate to Wikipedia</a></li>}$<br>\n",
    "$\\text{</ul>}$<br>\n",
    "$\\text{</div>}$<br>\n",
    "$\\text{</div>}$<br>\n",
    "</blockquote>\n",
    "The output of your solution should return the following list of tuples:<br><blockquote>\n",
    "$\\text{/wiki/Main_Page,Main page}$<br>\n",
    "$\\text{/wiki/Portal:Contents,Contents}$<br>\n",
    "$\\text{/wiki/Portal:Featured_content,Featured content}$<br>\n",
    "$\\text{/wiki/Portal:Current_events,Current events}$<br>\n",
    "$\\text{/wiki/Special:Random,Random article}$<br>\n",
    "$\\text{//donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&utm_medium=sidebar&utm_campaign=C13_en.wikipedia.org&uselang=en,Donate to Wikipedia}$<br>\n",
    "</blockquote>\n",
    "Hint: You may need 2 regexps, one for link and one for text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2SPEvcLiEJnF"
   },
   "outputs": [],
   "source": [
    "def link_text_detector(html):\n",
    "    \"\"\" Checks if parenthesis are balanced and properly ordered.\n",
    "    args:\n",
    "        html: str -- contains the html text\n",
    "    Outputs:\n",
    "        link_text_detector_list: list -- list of tupples (link ,text)\n",
    "    \"\"\"\n",
    "    link_text_detector_list = []\n",
    "    ## ***** START OF YOUR CODE **** ##\n",
    "    target_lines = re.compile(\"<a.+>.*</a>\").findall(html)\n",
    "    link_regex = re.compile(\"href=\\\"[^\\\"]+\\\"\")\n",
    "    text_regex = re.compile(\">[^<>]+</\")\n",
    "        \n",
    "    \n",
    "    for line in target_lines:\n",
    "        link = link_regex.findall(line)[0].replace(\"href=\\\"\", \"\").replace(\"\\\"\", \"\")\n",
    "        text = text_regex.findall(line)[0].replace(\">\", \"\").replace(\"</\", \"\")\n",
    "        link_text_detector_list += [(link, text)]\n",
    "    ## ***** END OF YOUR CODE ***** ##\n",
    "    return link_text_detector_list#returns tupple of url and text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "TRP6QAPVEJnI",
    "outputId": "188c9f20-9a3c-40c1-d70d-738019213701"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check Passed /wiki/Main_Page Main page\n",
      "Check Passed /wiki/Portal:Contents Contents\n",
      "Check Passed /wiki/Portal:Featured_content Featured content\n",
      "Check Passed /wiki/Portal:Current_events Current events\n",
      "Check Passed /wiki/Special:Random Random article\n",
      "Check Passed //donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&utm_medium=sidebar&utm_campaign=C13_en.wikipedia.org&uselang=en Donate to Wikipedia\n"
     ]
    }
   ],
   "source": [
    "# ** Code Below is to test your implementation of link_text_detector (Do Not Modify). ** #\n",
    "text=\"\"\"<div class=\"portal\" role=\"navigation\" id='p-navigation'>\n",
    "<h3>Navigation</h3>\n",
    "<div class=\"body\">\n",
    "<ul>\n",
    "<li id=\"n-mainpage-description\"><a href=\"/wiki/Main_Page\" title=\"Visit the main page [z]\" accesskey=\"z\">Main page</a></li>\n",
    "<li id=\"n-contents\"><a href=\"/wiki/Portal:Contents\" title=\"Guides to browsing Wikipedia\">Contents</a></li>\n",
    "<li id=\"n-featuredcontent\"><a href=\"/wiki/Portal:Featured_content\" title=\"Featured content the best of Wikipedia\">Featured content</a></li>\n",
    "<li id=\"n-currentevents\"><a href=\"/wiki/Portal:Current_events\" title=\"Find background information on current events\">Current events</a></li>\n",
    "<li id=\"n-randompage\"><a href=\"/wiki/Special:Random\" title=\"Load a random article [x]\" accesskey=\"x\">Random article</a></li>\n",
    "<li id=\"n-sitesupport\"><a href=\"//donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&utm_medium=sidebar&utm_campaign=C13_en.wikipedia.org&uselang=en\" title=\"Support us\">Donate to Wikipedia</a></li>\n",
    "</ul>\n",
    "</div>\n",
    "</div>\"\"\"\n",
    "returned_tupples=link_text_detector(text)\n",
    "\n",
    "check_tupples=[(\"/wiki/Main_Page\",\"Main page\")\n",
    ",(\"/wiki/Portal:Contents\",\"Contents\")\n",
    ",(\"/wiki/Portal:Featured_content\",\"Featured content\")\n",
    ",(\"/wiki/Portal:Current_events\",\"Current events\")\n",
    ",(\"/wiki/Special:Random\",\"Random article\")\n",
    ",(\"//donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&utm_medium=sidebar&utm_campaign=C13_en.wikipedia.org&uselang=en\",\"Donate to Wikipedia\")]\n",
    "try:\n",
    "    if(len(returned_tupples)!=len(check_tupples)):\n",
    "        print(\"Error: Length of tupple lists not equal\")\n",
    "    else:\n",
    "        for p1, p2 in zip(check_tupples, returned_tupples):\n",
    "            if(p1[0]==p2[0] and p1[1]==p2[1]):\n",
    "                print(\"Check Passed\",p1[0],p1[1])\n",
    "            else:\n",
    "                print(\"Test Failed\\nRequired: \",p1[0],p1[1],\"\\nGot:\",p2[0],p2[1])\n",
    "                break\n",
    "except:\n",
    "    print(\"Exception occured\")\n",
    "# ** Code Above is to test your implementation of link_text_detector (Do Not Modify). ** #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nXzr03SuEJnM"
   },
   "source": [
    "### iii. Word Finder\n",
    "Given some sentences and words, for each of these words you are to find the count of its occurences in all the sentences.<b>We define a word as a non-empty maximum sequence of characters that can contain only lowercase letters, uppercase letters, digits and underscores '_'.</b><br>\n",
    "Example:\n",
    "<blockquote>rare21 pepe (pepe) rare rare-pepe rare_pepe rare'pepe pepe123 pepe,rare.</blockquote>\n",
    "Occurance count for word <b>pepe</b>:\n",
    "<blockquote>5</blockquote>\n",
    "Explaination for occuranc count:\n",
    "<ul>\n",
    "<li>pepe is a single word(Counted)</li>\n",
    "<li>(pepe) is preceeded by '(' and followed by ')', so it's the second word(Counted)</li>\n",
    "<li>rare-pepe is considered as two words and 'pepe' is the second word(Counted)</li>\n",
    "<li>rare_pepe is a single single word(NotCounted)</li>\n",
    "<li>pepe123 is a single single word(NotCounted)</li>\n",
    "<li>rare'pepe is considered as two words and 'pepe' is the second word(Counted)</li>\n",
    "<li>pepe, as it is preceeded by a space and followed by a dot ','(Counted)</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cEQ1lvjUEJnN"
   },
   "outputs": [],
   "source": [
    "def word_finder(sentences,words):\n",
    "    \"\"\" Checks if parenthesis are balanced and properly ordered.\n",
    "    args:\n",
    "        sentences: list -- list of sentences\n",
    "        words: list -- list of words\n",
    "    Outputs:\n",
    "        word_finder_list: list -- list of tupples (word,count)\n",
    "    \"\"\"\n",
    "    word_finder_list = []\n",
    "    ## ***** START OF YOUR CODE **** ##\n",
    "    for word in words:\n",
    "        count = 0\n",
    "        reg = \"(((?<![_a-zA-Z0-9\\.])\"+word+\"|(^\"+word+\"))([^_a-zA-Z0-9]))\"\n",
    "        r = re.compile(reg)\n",
    "        for sentence in sentences:\n",
    "            tmp = r.findall(sentence)\n",
    "            count += len(tmp)\n",
    "        word_finder_list += [(word, count)]\n",
    "    ## ***** END OF YOUR CODE ***** ##\n",
    "    return word_finder_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GsDOsc5uEJnQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 3\n",
      "Check Passed pepe 5\n",
      "Check Passed foo 7\n",
      "Check Passed bamboozled 4\n"
     ]
    }
   ],
   "source": [
    "# ** Code Below is to test your implementation of word_finder (Do Not Modify). ** #\n",
    "sentences=[\"rare21 pepe (rare) pepe rare-pepe rare_pepe foo'pepe pepe123 pepe,rare.\",\n",
    "        \"foo bar (foo) bar foo-bar foo_bar foo'bar bar-foo bar, foo.\",\n",
    "        \"bamboozled bamboozled-doggo bamboozled_doggo doggo bamboozled bamboozled2 bamboozled*doggo\"]\n",
    "words=[\"pepe\",\"foo\",\"bamboozled\"]\n",
    "counts=[(\"pepe\",5),(\"foo\",7),(\"bamboozled\",4)]\n",
    "output=word_finder(sentences,words)\n",
    "try:\n",
    "    print(len(counts),len(output))\n",
    "    if(len(counts)!=len(output)):\n",
    "        print(\"Error: Length of tupple lists not equal\")\n",
    "    else:\n",
    "        for p1, p2 in zip(counts, output):\n",
    "            if(p1[0]==p2[0] and p1[1]==p2[1]):\n",
    "                print(\"Check Passed\",p1[0],p1[1])\n",
    "            else:\n",
    "                print(\"Test Failed\\nRequired: \",p1[0],p1[1],\"\\nGot:\",p2[0],p2[1])\n",
    "                break\n",
    "except:\n",
    "    print(\"Exception occured\")\n",
    "# ** Code Above is to test your implementation of word_finder (Do Not Modify). ** #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1Rb4QpXTEJnT"
   },
   "source": [
    "## 2) Minimum Edit Distance\n",
    "You are to design a general purpose solution for minimum edit distance that works for different values of insertion, deletion, subsitution. <a href=\"https://web.stanford.edu/class/cs124/lec/med.pdf\">Slides</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oOEmE4VeEJnU"
   },
   "outputs": [],
   "source": [
    "def make_table(source, target, cost_insertion, cost_deletion):\n",
    "    matrix = [[0 for x in range(len(source) + 2)] for x in range(len(target) + 2)] \n",
    "    tmp = len(target)-1\n",
    "    for i in matrix:\n",
    "        if tmp == -2:\n",
    "            i[0] = \"#\"\n",
    "            i[1] = \"#\"\n",
    "            ind = 2\n",
    "            for j in source:\n",
    "                i[ind] = j\n",
    "                ind += 1\n",
    "        if tmp == -1:\n",
    "            i[0] = \"#\"\n",
    "            ind = 1 \n",
    "            for j in range(0, len(source)+1):\n",
    "                i[ind] = j*cost_deletion\n",
    "                ind += 1\n",
    "            tmp -= 1\n",
    "        if tmp >= 0:\n",
    "            i[0] = target[tmp]\n",
    "            i[1] = (tmp+1)*cost_insertion\n",
    "            tmp -= 1\n",
    "    return matrix\n",
    "def printMatrix(matrix):\n",
    "    for ele in matrix:\n",
    "        print(ele)\n",
    "def minimum_edit_distance(source,target,cost_insertion=1,cost_deletion=1,cost_subsitution=1):\n",
    "    \"\"\" Implement the minimum edit distance algorithm.\n",
    "    args:\n",
    "        source: str -- source string\n",
    "        target: str -- target string\n",
    "        cost_insertion: int -- cost of insertion\n",
    "        cost_subsitution: int -- cost of subsitution\n",
    "        cost_deletion: int -- cost of deletion\n",
    "    Outputs:\n",
    "        min_edit_dist: int -- minimum edit distance\n",
    "    \"\"\"\n",
    "    min_edit_dist = 0\n",
    "    ## ***** START OF YOUR CODE **** ##\n",
    "    matrix = make_table(source, target, cost_insertion, cost_deletion)\n",
    "    for i in reversed(range(0, len(matrix)-2)):\n",
    "        for j in range(2, len(matrix[i])):\n",
    "            third = matrix[i+1][j-1]\n",
    "            if matrix[i][0] != matrix[len(matrix)-1][j]:\n",
    "                third += cost_subsitution\n",
    "            matrix[i][j] = min(matrix[i][j-1]+cost_deletion, matrix[i+1][j]+cost_insertion, third)\n",
    "    ## ***** END OF YOUR CODE ***** ##\n",
    "    return matrix[0][len(matrix[0])-1]\n",
    "# minimum_edit_distance(\"ABCD\", \"ACD\")\n",
    "# print(minimum_edit_distance(\"intention\",\"execution\",2,2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CGWu2JqMEJnX"
   },
   "outputs": [],
   "source": [
    "# ** Code Below is to test your implementation of minimum_edit_distance (Do Not Modify). ** #\n",
    "test_cases=[(\"intention\",\"execution\",1,1,2,8),(\"intention\",\"execution\",1,1,1,5),(\"trails\",\"ziel\",1,1,2,6),(\"intention\",\"execution\",2,2,2,10),(\"trails\",\"ziel\",1,2,2,9),(\"trails\",\"ziel\",2,1,1,5)]\n",
    "for case in test_cases:\n",
    "    output=minimum_edit_distance(case[0],case[1],case[2],case[3],case[4])\n",
    "    if(case[5]==output):\n",
    "        continue\n",
    "    else:\n",
    "        print(\"failed, for source:\",case[0],\" and target:\",case[1],\"\\nintertion,deletion and subsitution cost:\",case[2],case[3],case[4],\"\\nExpected output\",case[5],\" Got:\",output)\n",
    "        break\n",
    "# ** Code Above is to test your implementation of minimum_edit_distance (Do Not Modify). ** #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wd-JmKN0EJnZ"
   },
   "source": [
    "## 3) NLTK\n",
    "In this part of the assignment you will be introduced to natural language toolkit or nltk. NLTK is the swiss army knife of natural language processing it contains many corporas, basic machine learning algorithm and text preprocessing techniques.  But it is primarily used for preprocessing step in the machine learning pipeline.\n",
    "This part of the assignment is meant to be an introduction to the following form nltk:\n",
    "<ul>\n",
    "<li>Tokenization</li>\n",
    "<li>Stopwords removal</li>\n",
    "<li>Stemming</li>\n",
    "<li>Lemmatization</li>\n",
    "<li>Frequence Distribution</li>\n",
    "</ul>\n",
    "First run the command in the cell below, it should open a GUI from where you can download different packages, collections ,models and corporas. For now, install the book collection. You can also use the command <b>nltk.download('package_name')</b> instead of using the GUI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rIGwCxjSEJna"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hqSTHiAlEJnc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qb9vaGg_EJnf"
   },
   "source": [
    "The texts are stored in the varables text1,text2 and so on, we will be using text 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GeBX6IMUEJng"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building ngram index...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "laid by her , and said unto Cain , Where art thou , and said , Go to ,\n",
      "I will not do it for ten ' s sons ; we dreamed each man according to\n",
      "their generatio the firstborn said unto Laban , Because I said , Nay ,\n",
      "but Sarah shall her name be . , duke Elah , duke Shobal , and Akan .\n",
      "and looked upon my affliction . Bashemath Ishmael ' s blood , but Isra\n",
      "for as a prince hast thou found of all the cattle in the valley , and\n",
      "the wo The\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"laid by her , and said unto Cain , Where art thou , and said , Go to ,\\nI will not do it for ten ' s sons ; we dreamed each man according to\\ntheir generatio the firstborn said unto Laban , Because I said , Nay ,\\nbut Sarah shall her name be . , duke Elah , duke Shobal , and Akan .\\nand looked upon my affliction . Bashemath Ishmael ' s blood , but Isra\\nfor as a prince hast thou found of all the cattle in the valley , and\\nthe wo The\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text3.generate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kSVUcr5ZEJni"
   },
   "source": [
    "Although the code below is redundant, we want to to use <b>tokenizer from nltk to tokenize the data</b>. The data by default is in tokenized form as demonstrated below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HdTWCLEKEJnj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In', 'the', 'beginning', 'God', 'created', 'the', 'heaven', 'and', 'the', 'earth', '.', 'And', 'the', 'earth', 'was']\n"
     ]
    }
   ],
   "source": [
    "corpora=\" \".join(text3)#joins the token based on space delimiter\n",
    "print(text3[:15])#prints the first fifteen tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2TwhBKdQEJnm"
   },
   "source": [
    "For all of tasks below consult the documentation <a href=\"https://www.nltk.org/book/ch01.html\">here</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7iq3Kn3vEJnn"
   },
   "source": [
    "### a)Tokenization\n",
    "Tokenize text3 and print the first 20 tokens. For those of you who are new to python its a good time to learn about <a href=\"https://railsware.com/blog/python-for-machine-learning-indexing-and-slicing-for-lists-tuples-strings-and-other-sequential-types/\">array slices</a>.\n",
    "<br><a href=\"https://www.nltk.org/api/nltk.tokenize.html\">Tokenize Documentation</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X4bmlsT8EJnp"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yrd4RMuoEJnr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In', 'the', 'beginning', 'God', 'created', 'the', 'heaven', 'and', 'the', 'earth', '.', 'And', 'the', 'earth', 'was', 'without', 'form', ',', 'and', 'void']\n"
     ]
    }
   ],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\" Tokenize the text using word_tokenize\n",
    "    args:\n",
    "        text: str -- corpus\n",
    "    Outputs:\n",
    "        tokens: list of word tokens\n",
    "    \"\"\"\n",
    "    tokens=[]\n",
    "    ## ***** START OF YOUR CODE **** ##\n",
    "    tokens = word_tokenize(text)[:20]\n",
    "    ## ***** END OF YOUR CODE ***** ##\n",
    "    return tokens\n",
    "    \n",
    "tokenized=tokenize(corpora)\n",
    "#add code here to print first 20 tokens\n",
    "print(tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vBL5HG2eEJnt"
   },
   "source": [
    "### b)Total count of  tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RlvQToDOEJnu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total token count 20\n"
     ]
    }
   ],
   "source": [
    "def token_count(tokens):\n",
    "    \"\"\" Total count of tokens\n",
    "    args:\n",
    "        tokens: list of word tokens\n",
    "    Outputs:\n",
    "        count : int -- number of total tokens (integer)\n",
    "    \"\"\"\n",
    "    count=0\n",
    "    ## ***** START OF YOUR CODE **** ##\n",
    "    count = len(tokens)\n",
    "    ## ***** END OF YOUR CODE ***** ##\n",
    "    return count #return token count\n",
    "\n",
    "total_tokens=token_count(tokenized)\n",
    "print(\"Total token count\",total_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Az5LQ4sGEJnw"
   },
   "source": [
    "### c)Total count of unique tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6k9tTrkgEJny"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique token count 15\n"
     ]
    }
   ],
   "source": [
    "def token_count_unique(tokens):\n",
    "    \"\"\" Total count of unique tokens\n",
    "    args:\n",
    "        tokens: list of word tokens\n",
    "    Outputs:\n",
    "        count : int -- number of total unique tokens (integer)\n",
    "    \"\"\"\n",
    "    unique_count=0\n",
    "    ## ***** START OF YOUR CODE **** ##\n",
    "    unique_count = len(set(tokens))\n",
    "    ## ***** END OF YOUR CODE ***** ##\n",
    "    return unique_count#return unique token count\n",
    "total_tokens_unique=token_count_unique(tokenized)\n",
    "print(\"Total unique token count\",total_tokens_unique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QCXGPEnjEJn0"
   },
   "source": [
    "### d)Vocabulary\n",
    "Print the entire vocabilary, here you should keep the definition of vocabulary in mind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YvN23xOdEJn1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.', 'the', 'form', 'was', 'heaven', 'And', 'created', 'void', 'beginning', 'God', 'In', 'earth', ',', 'and', 'without']\n"
     ]
    }
   ],
   "source": [
    "def vocabulary(tokens):\n",
    "    \"\"\" Print entire vocabulary\n",
    "    args:\n",
    "        tokens: list of word tokens\n",
    "    Outputs:\n",
    "        list : vocabulary(tokens)\n",
    "    \"\"\"\n",
    "    ## ***** START OF YOUR CODE **** ##\n",
    "    return list(set(tokens))\n",
    "    ## ***** END OF YOUR CODE ***** ##\n",
    "print(vocabulary(tokenized))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7BpS0qBDEJn4"
   },
   "source": [
    "### e)Frequency Distribution\n",
    "Print the Frequency Distribution for the top 50 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c7ZM49LgEJn4"
   },
   "outputs": [],
   "source": [
    "from nltk import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_yrwM9UoEJn6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 4),\n",
       " ('and', 2),\n",
       " ('earth', 2),\n",
       " ('In', 1),\n",
       " ('beginning', 1),\n",
       " ('God', 1),\n",
       " ('created', 1),\n",
       " ('heaven', 1),\n",
       " ('.', 1),\n",
       " ('And', 1),\n",
       " ('was', 1),\n",
       " ('without', 1),\n",
       " ('form', 1),\n",
       " (',', 1),\n",
       " ('void', 1)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def frequency_distribution(tokens):\n",
    "    \"\"\" Print entire vocabulary\n",
    "    args:\n",
    "        tokens: list of word tokens\n",
    "    Outputs:\n",
    "         frequency_distribution: FreqDist -- frequency disribution\n",
    "    \"\"\"\n",
    "    ## ***** START OF YOUR CODE **** ##\n",
    "#     dist = {}\n",
    "#     for ele in tokens:\n",
    "#         if ele not in dist: dist[ele] = 0\n",
    "#         dist[ele] += 1\n",
    "    return FreqDist(tokens)\n",
    "    ## ***** END OF YOUR CODE ***** ##\n",
    "#     return _frequency_distribution\n",
    "    \n",
    "freqdist=frequency_distribution(tokenized)\n",
    "#add code here to print top 50 words\n",
    "freqdist.most_common(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z7cm-qsZEJoC"
   },
   "source": [
    "### f) Bigram Frequence Distribution\n",
    "Print the Bigram Frequence Distribution for top 50 bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bUOWw9_vEJoD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('the', 'earth'), 2), (('In', 'the'), 1), (('the', 'beginning'), 1), (('beginning', 'God'), 1), (('God', 'created'), 1), (('created', 'the'), 1), (('the', 'heaven'), 1), (('heaven', 'and'), 1), (('and', 'the'), 1), (('earth', '.'), 1), (('.', 'And'), 1), (('And', 'the'), 1), (('earth', 'was'), 1), (('was', 'without'), 1), (('without', 'form'), 1), (('form', ','), 1), ((',', 'and'), 1), (('and', 'void'), 1)]\n"
     ]
    }
   ],
   "source": [
    "def frequency_distribution_bigram(text):\n",
    "    \"\"\" Print entire vocabulary\n",
    "    args:\n",
    "        tokens: list of word tokens\n",
    "    Outputs:\n",
    "        frequency_distribution_bigram: FreqDist -- frequency distribution of bigrams\n",
    "    \"\"\"\n",
    "    ## ***** START OF YOUR CODE **** ##\n",
    "    bgs = nltk.bigrams(text)\n",
    "    dist = FreqDist(bgs)\n",
    "    return dist\n",
    "    ## ***** END OF YOUR CODE ***** ##\n",
    "    return frequency_distribution_bigram#returns a Frequency Distribution of bigrams\n",
    "\n",
    "freqdist_bigram=frequency_distribution_bigram(tokenized)\n",
    "#add code here to print top 50 bigrams\n",
    "print(freqdist_bigram.most_common(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y-rBpzlnEJoG"
   },
   "source": [
    "### f)Compute lexical diversity\n",
    "Here we will define lexical diversity as the ratio between the number of unique tokens and total number of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X-o2ysGyEJoG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lexical diversity score 0.75\n"
     ]
    }
   ],
   "source": [
    "def lexical_diversity(tokens):\n",
    "    \"\"\" Print entire vocabulary\n",
    "    args:\n",
    "        tokens: list of word tokens\n",
    "    Outputs:\n",
    "        diversity_score: float -- lexical diversity \n",
    "    \"\"\"\n",
    "    ## ***** START OF YOUR CODE **** ##\n",
    "    return len(set(tokens))/len(tokens)\n",
    "    ## ***** END OF YOUR CODE ***** ##\n",
    "    return diversity_score\n",
    "\n",
    "lexical_diversity_score=lexical_diversity(tokenized)\n",
    "print(\"Lexical diversity score\",lexical_diversity_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wTNzsicUEJoJ"
   },
   "source": [
    "### g)Remove Stopwords\n",
    "Read the documentation and remove the <a href=\"https://en.wikipedia.org/wiki/Stop_words\">stopwords</a> in `tokenized` list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z4Sb3ktWEJoJ"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wf14EDbNEJoM"
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(tokens):\n",
    "    \"\"\" Remove stopwords\n",
    "    args:\n",
    "        tokens: list -- list of word tokens\n",
    "    Outputs:\n",
    "        stopwords_removed_tokens: list -- list of tokens with stopwords removed\n",
    "    \"\"\"\n",
    "    ## ***** START OF YOUR CODE **** ##\n",
    "    return [x for x in tokens if x not in stopwords.words()]\n",
    "    ## ***** END OF YOUR CODE ***** ##\n",
    "    return stopwords_removed_tokens\n",
    "tokenized_stopword=remove_stopwords(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2_SzKfbGr63_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In', 'beginning', 'God', 'created', 'heaven', 'earth', '.', 'And', 'earth', 'without', 'form', ',', 'void']\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_stopword)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KgKuGmDpEJoO"
   },
   "source": [
    "### h)Lemmatization\n",
    "Apply <a href=\"https://en.wikipedia.org/wiki/Lemmatisation\">Lemmatization</a> the words in `tokenized` list and see its effects on the token list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eevtSKUhEJoP"
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W2ep0yIwEJoS"
   },
   "outputs": [],
   "source": [
    "def lemmatization(tokens):\n",
    "    \"\"\" Apply lemamatization\n",
    "    args:\n",
    "        tokens: list -- list of word tokens\n",
    "    Outputs:\n",
    "        lemmatized_tokens: list -- list of lemmatized word tokens\n",
    "    \"\"\"\n",
    "    ## ***** START OF YOUR CODE **** ##\n",
    "    l = WordNetLemmatizer()\n",
    "    return [l.lemmatize(x) for x in tokens]\n",
    "    ## ***** END OF YOUR CODE ***** ##\n",
    "    return lemmatized_tokens\n",
    "lemmatized_tokens=lemmatization(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LaYsy1Pkr64H"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In', 'the', 'beginning', 'God', 'created', 'the', 'heaven', 'and', 'the', 'earth', '.', 'And', 'the', 'earth', 'wa', 'without', 'form', ',', 'and', 'void']\n"
     ]
    }
   ],
   "source": [
    "print(lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n9cjn2TZEJoW"
   },
   "source": [
    "### i)Stemming\n",
    "Apply <a href=\"https://en.wikipedia.org/wiki/Stemming\">Stemming</a> to the words in `tokenized` list list and see its effects on the token list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KbeFobzdEJoW"
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fUX2NsQcEJoY"
   },
   "outputs": [],
   "source": [
    "def stemming(tokens):\n",
    "    \"\"\" Apply Stemming\n",
    "    args:\n",
    "        tokens: list -- list of word tokens\n",
    "    Outputs:\n",
    "        stemmed_tokens: list -- list of stemmed tokens \n",
    "    \"\"\"\n",
    "    ## ***** START OF YOUR CODE **** ##\n",
    "    ps = PorterStemmer()\n",
    "    return [ps.stem(x) for x in tokens]\n",
    "    ## ***** END OF YOUR CODE ***** ##\n",
    "    return stemmed_tokens\n",
    "stemmed_tokens=stemming(tokenized)\n",
    "tokenized_preprocessed=stemmed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MqVZ92Lar64P"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In', 'the', 'begin', 'god', 'creat', 'the', 'heaven', 'and', 'the', 'earth', '.', 'and', 'the', 'earth', 'wa', 'without', 'form', ',', 'and', 'void']\n"
     ]
    }
   ],
   "source": [
    "print(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gUL6WBWTEJoa"
   },
   "source": [
    "### j)Recompute\n",
    "Now repeat parts b through e for the new `lemmatized_tokens` list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IUK6E7mYEJob"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total token count 20\n"
     ]
    }
   ],
   "source": [
    "#part b\n",
    "total_tokens=token_count(lemmatized_tokens)\n",
    "print(\"Total token count\",total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "58U8a6WYEJod"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique token count 15\n"
     ]
    }
   ],
   "source": [
    "#part c\n",
    "total_tokens_unique=token_count_unique(lemmatized_tokens)\n",
    "print(\"Total unique token count\",total_tokens_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rF53voG0EJog"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.', 'the', 'form', 'void', 'heaven', 'And', 'created', 'beginning', 'God', 'In', 'earth', 'wa', ',', 'and', 'without']\n"
     ]
    }
   ],
   "source": [
    "#part d\n",
    "print(vocabulary(lemmatized_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xcIpa85kEJoj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('the', 'earth'), 2), (('In', 'the'), 1), (('the', 'beginning'), 1), (('beginning', 'God'), 1), (('God', 'created'), 1), (('created', 'the'), 1), (('the', 'heaven'), 1), (('heaven', 'and'), 1), (('and', 'the'), 1), (('earth', '.'), 1), (('.', 'And'), 1), (('And', 'the'), 1), (('earth', 'wa'), 1), (('wa', 'without'), 1), (('without', 'form'), 1), (('form', ','), 1), ((',', 'and'), 1), (('and', 'void'), 1)]\n"
     ]
    }
   ],
   "source": [
    "#part e\n",
    "freqdist_bigram=frequency_distribution_bigram(lemmatized_tokens)\n",
    "print(freqdist_bigram.most_common(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rMaOTfaWEJol"
   },
   "source": [
    "## k) Lemmatization vs Stemming\n",
    "You have learnt about [stemming](https://en.wikipedia.org/wiki/stemming) and [lemmatization](https://en.wikipedia.org/wiki/Lemmatisation) in class, usually one of them is used according to requirements of the task you are trying to solve. Discuss in what situations might one be preferable over the other? <a href=\"https://ibb.co/p3f4Wtd\">In other words</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N_Nnrhq-EJom"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n## ***** START OF YOUR ANSWER **** ##\\n\\nLemmatization is useful in the applications where morphological analysis under the\\nmeaningful linguistic context of the tokens is taken into consideration for example the \\nquestion \"Does the following two sentences talk about same thing?\" would require the\\nanalysis if the two sentences while considering the meaningful context of the two.\\n\\nStemming is useful when the applications have loose requirement of the meaningful \\nlinguistic context of the tokens. Due to the  loose requirements, a light-weight \\ntechnique, stemming, can be used instead of the sophisticated lemmatization. Examples \\ninclude \"How much vocabulary did Shakespear use?\"\\n\\n## ***** END OF YOUR ANSWER **** ##\\n'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "## ***** START OF YOUR ANSWER **** ##\n",
    "\n",
    "Lemmatization is useful in the applications where morphological analysis under the\n",
    "meaningful linguistic context of the tokens is taken into consideration for example the \n",
    "question \"Does the following two sentences talk about same thing?\" would require the\n",
    "analysis if the two sentences while considering the meaningful context of the two.\n",
    "\n",
    "Stemming is useful when the applications have loose requirement of the meaningful \n",
    "linguistic context of the tokens. Due to the  loose requirements, a light-weight \n",
    "technique, stemming, can be used instead of the sophisticated lemmatization. Examples \n",
    "include \"How much vocabulary did Shakespear use?\"\n",
    "\n",
    "## ***** END OF YOUR ANSWER **** ##\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0FDy16bcr64i"
   },
   "source": [
    "Great you're done with part 1; Now, move onto <b><a href=\"https://ibb.co/MfpxHZh\"> part 2</a></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oJP5-RlOr64i"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great\n"
     ]
    }
   ],
   "source": [
    "print(\"Great\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "7iq3Kn3vEJnn",
    "vBL5HG2eEJnt",
    "Az5LQ4sGEJnw",
    "QCXGPEnjEJn0",
    "7BpS0qBDEJn4",
    "Z7cm-qsZEJoC",
    "Y-rBpzlnEJoG",
    "wTNzsicUEJoJ",
    "KgKuGmDpEJoO",
    "n9cjn2TZEJoW",
    "gUL6WBWTEJoa"
   ],
   "name": "Assignment1_notebook1_[student_ID].ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
