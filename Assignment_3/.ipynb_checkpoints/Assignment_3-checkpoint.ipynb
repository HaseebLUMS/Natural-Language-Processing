{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3 CS 5316 Natural Language Processing\n",
    "For this assignment we will use the following packages\n",
    "<ul>\n",
    "    <li><a href=\"https://radimrehurek.com/gensim/\">Gensim</a>.</li>\n",
    "    <li><a href=\"https://keras.io/\">Keras</a>.</li>\n",
    "    <li><a href=\"https://www.tensorflow.org/\">Tensorflow</a>.</li>\n",
    "</ul>\n",
    "You can install these packages via anaconda navigator or use the conda install / pip install commands e.g<br>\n",
    "<blockquote>pip install gensim<br>\n",
    "pip install tensorflow<br>\n",
    "pip install keras</blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from IPython.display import Image\n",
    "# Get the interactive Tools for Matplotlib\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "from sklearn.decomposition import PCA\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.utils.extmath import randomized_svd\n",
    "import nltk\n",
    "from nltk import ngrams\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings\n",
    "\n",
    "Word Vectors nowadays are often used as a fundamental component for downstream NLP tasks, e.g. question answering, text generation, language translation, sentiment analysis, etc. The goal of word embedding methods is to derive a low-dimensional continuous vector representation for words so that words that are syntactically or semantically related are close together in that vector space and thus, share a similar representation.\n",
    "\n",
    "In this assingment we are going to explore different word embedddings inorder to build some intuitions about their strengths and weaknesses. Although there are many types of word embeddings they can be broadly classified into two categories:\n",
    "<ul>\n",
    "    <li>Frequency based Embedding</li>\n",
    "    <li>Prediction based Embedding</li>\n",
    "</ul>\n",
    "For frequenct based embedding we will explore embeddings based on <b>word co-occurance</b> counts with <a href=\"https://en.wikipedia.org/wiki/Pointwise_mutual_information\">Point Wise Mutial Information(PPMI)</a> and <a href=\"https://en.wikipedia.org/wiki/Singular_value_decomposition\">Singular Value Decomposition(SVD)</a>.\n",
    "<a href=\"https://www.youtube.com/watch?v=P5mlg91as1c\">SVD video explaination</a><br>\n",
    "For prediction based embeddings we will explore <a href=\"https://en.wikipedia.org/wiki/Pointwise_mutual_information\">Word2Vec</a> based embeddings.\n",
    "\n",
    "For evaluating these embeddings we will work with the following two datasets: \n",
    "<ul>\n",
    "    <li>Twitter dataset created by Sanders Analytics which we explored in the previous assignment<b>(file provided)</b></li>\n",
    "    <li>Movie reviews dataset from the popular website <a href=\"https://www.imdb.com/\">IMDB</a>.\n",
    "        Head over the to <a href=\"https://www.kaggle.com/c/word2vec-nlp-tutorial/data\">kaggle</a> and download the dataset from there. The dataset consists of three files:<br><b>labelledTrainData,unlabelledTrainData,testData</b></li>   \n",
    "</ul>\n",
    "Read the \"Data\" section on kaggle for details on the dataset.\n",
    "\n",
    "Let's get started.......<br>\n",
    "remove this link later [Assignment solution](https://www.youtube.com/watch?v=dQw4w9WgXcQ&t=40s) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency base Embeddings\n",
    "For this part we will use the Sanders Analytics dataset to create embeddings. Since the other dataset is large we might run into memory problems.<br><br><br>\n",
    "Although we can directly use word representation based on word co-occurance matrix directly it is generally not a good idea to do so for the following reasons:\n",
    "<ul>\n",
    "    <li>The word co-occurance matrix scales with vocabulary size, considering memory constraints this would be problematic for large datasets, as in the IMDB data set that has vocabulary size after remove stop words of 225109, which requres rougly around 189 GiB of storage capacity(roughly 203 GB)</li><img src=\"memoryerror.png\">\n",
    "    <li> The word co-occurance matrix will be quite sparse, meaning many entries in the matrix will be zeros. This is problematic due the fact that for many nlp tasks the multyplication operation is used quite frequently, e.g. for word similarity task, cosine similarity is used:<img src=\"cosine-equation.png\"> Here we can see the dot product is computed between two word vectors, multyplication with zeros wastes precious computation power and your time.</li>\n",
    "    <li> High co-occurance counts for stop words and conjunctions offset true representation of words, meaning thier could become a dominant factor when these embeddings are used in computations. These also dont provide a lot of information as thier counts with other words would also be high.</li>\n",
    "</ul>\n",
    "In summary, you want to avoid sparse represenation's just like the corona virus.<br>\n",
    "To mitigate the above problems we will use PPMI and SVD. PPMI is use to control high co-occurance counts and SVD is used to reduce dimensionality.\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "Since we have already discussed preprocessing trade off's in previous assingments. We expect you to analyse the data and preform the preprocessing that is required.<br> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "988 988\n"
     ]
    }
   ],
   "source": [
    "#load the dataset\n",
    "def load_data(filename):\n",
    "    \"\"\"\n",
    "    Load data from file\n",
    "\n",
    "    Args:\n",
    "        filename : Name of the file from which the data is to be loaded\n",
    "    \n",
    "    Returns: tweet_X, sentiment_Y\n",
    "    tweet_X: list of tweets\n",
    "    sentiment_Y: list of sentiment lables correponding to each tweet\n",
    "    \"\"\"\n",
    "    data = pd.read_csv(filename)\n",
    "    X = data[\"text\"]\n",
    "    Y = data[\"class\"]\n",
    "    return X, Y\n",
    "raw_tweets, labels = load_data(\"twitter-sanders-apple3.csv\")\n",
    "print(len(raw_tweets), len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1317\n",
      "988 ['now', 'all', 'AT_TOKEN', 'has', 'to', 'do', 'is', 'get', 'UNK', 'on', 'the', 'iphone', 'and', 'it', 'will', 'be', 'crack', 'iphone', 'that', 'is']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def preprocess(tweet):\n",
    "    processed_data = tweet.lower()\n",
    "    processed_data = re.sub(\"@[\\S]+\", \"AT_TOKEN\", processed_data)\n",
    "    processed_data = re.sub(\"[#]\", \"\", processed_data)\n",
    "    processed_data = re.sub(\"<.*>\", \"\", processed_data)\n",
    "    processed_data = re.sub(\"[^a-zA-Z0-9\\s_]\", \" \", processed_data)\n",
    "    processed_data = re.sub(\"[\\s]+\", \" \", processed_data)\n",
    "    processed_data = re.sub(\"[_]{2,10}\", \"\", processed_data)\n",
    "    processed_data = processed_data.split(\" \")\n",
    "    return [x for x in processed_data if len(x)]\n",
    "def preprocessing(data):\n",
    "    \"\"\"\n",
    "    Perform preprocessing of the tweets\n",
    "\n",
    "    Args:\n",
    "        data : list of tweets\n",
    "    \n",
    "    Returns: data: preprocessed list of tweets\n",
    "    \"\"\"\n",
    "#     return [preprocess(data[0])]\n",
    "    new_data = [preprocess(x) for x in data]\n",
    "    tokens = list(sum(new_data, []))\n",
    "    for i in range(0, len(new_data)):\n",
    "        for j in range(0, len(new_data[i])):\n",
    "            if tokens.count(new_data[i][j]) == 1:\n",
    "                new_data[i][j] = 'UNK'\n",
    "    print(len(set(sum(new_data, []))))\n",
    "    return new_data\n",
    "    \n",
    "tweets = preprocessing(raw_tweets)\n",
    "print(len(tweets), tweets[0])\n",
    "X_train = tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test train split\n",
    "Use test train split from sklearn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['UNK', 'that', 'my', 'dad', 'loves', 'my', 'UNK', 'more', 'rt', 'AT_TOKEN', 'UNK', 'up', 'to', 'my', 'dad', 'at', 'my', 'door', 'w', 'my', 'new', 'iphone', 'he', 'surprised', 'me', 'went', 'to', 'AT_TOKEN']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "def testTrainSplit(data_X,data_Y):\n",
    "    \"\"\"\n",
    "    Return test train data\n",
    "\n",
    "    Args:\n",
    "        data_X : reviews\n",
    "        data_Y: labels\n",
    "    Returns: test train split data \n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(tweets, labels, test_size=0.3, random_state=0)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "X_train, X_test, y_train, y_test = testTrainSplit(tweets, labels)\n",
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the vocabulary, to find te dimensions of co-occurance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1265\n"
     ]
    }
   ],
   "source": [
    "def getVocabulary(train_X):\n",
    "    \"\"\"\n",
    "    Return dataset vocabulart\n",
    "\n",
    "    Args:\n",
    "        train_X : reviews in train dataset\n",
    "    \n",
    "    Returns: vocabulary\n",
    "    vocabulary: list of unique words in dataset\n",
    "    \"\"\"\n",
    "    vocab = list(set(list(sum(train_X, []))))\n",
    "    return vocab\n",
    "vocabulary = getVocabulary(X_train)\n",
    "print(len(vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Point Wise Mutial Information\n",
    "Pointwise mutual information, or PMI, is the (unweighted) term that occurs inside of the summation of mutual information and measures the correlation between two specific events. Specifically, PMI is defined as<br>\n",
    "$$PMI(a, b) = \\log \\frac{p(a,b)}{p(a)p(b)}$$\n",
    "\n",
    "and measures the (log) ratio of the joint probability of the two events as compared to the joint probability of the two events assuming they were independent. Thus, PMI is high when the two events a and b co-occur with higher probability than would be expected if they were independent.\n",
    "\n",
    "If we suppose that a and b are words, we can measure how likely we see a and b together compared to what we would expect of they were unrelated by computing their PMI under some model for the joint probability $$p(a,b)$$\n",
    "\n",
    "Let D represent a collection of observed word-context pairs (with contexts being other words). We can construct D by considering the full context of a specific word occurrence as the collection of all word occurrences that appear within a fixed-size window of length L before and after it.\n",
    "\n",
    "For a specific word $w_i$ in position i in a large, ordered collection of words $w_1, w_2$, we would have the context as ,$w_{i-1},w_{i+1},\\ldots$, and could thus collect counts (a total of 2L) of each of those words as appearing in the context of word $w_i$. We will refer to $w_i$ as the “target word” and the words appearing in the L-sized window around $w_i$ as “context words”.\n",
    "\n",
    "Consider a sample corpus containing only one sentence:<br>\n",
    "    <center><blockquote>Encumbered forever by desire and ambition</blockquote></center>\n",
    "\n",
    "We can construct D by considering each word position i and extracting the pairs $(w_i, w_{i+k})$ for $−L≤k≤L;k≠0$. In such a pair, we would call $w_i$ the “target word” and $w_{i+k}$ the “context word”.\n",
    "\n",
    "For example, we would extract the following pairs for $i=4i$ if we let our window size $L=2$<br>\n",
    "    <center><blockquote>(desire,forever),(desire,by),(desire,and),(desire,ambition)</blockquote></center>\n",
    "\n",
    "Similarly, for $i=5i$, we would extract the following pairs:\n",
    "    <center><blockquote>(and,by),(and,desire),(and,ambition)</blockquote></center>\n",
    "Let’s let $n_{w,c}$ represent the number of times we observe word type c in the context of word type w. We can then define ,$n_w = \\sum_{c'} n_{w,c'}$ as the number of times we see a “target” word w in the collection of pairs D and $n_c = \\sum_{w'} n_{w',c}$ as the number of times we see the context word c in the collection of pairs D.\n",
    "\n",
    "We can then define the joint probability of a word and a context word as\n",
    "    $$p(w, c) = \\frac{n_{w,c}}{|D|}$$\n",
    "\n",
    "where $∣D∣$ is simply the total number of word-context occurrences we see. Similarly, we can define\n",
    "    $$p(w) = \\frac{n_w}{|D|}$$\n",
    "\n",
    "and $$p(c) = \\frac{n_c}{|D|}$$\n",
    "\n",
    "and thus the PMI between a word w and context word c is\n",
    "$$PMI(w, c) = \\log \\frac{p(w,c)}{p(w)p(c)} = \\log \\frac{n_{w,c} \\cdot |D|}{n_w \\cdot n_c}.$$\n",
    "\n",
    "If we compute the PMI between all pairs of words in our vocabulary V, we will arrive at a large, real-valued matrix. However, some of the values of this matrix will be $\\log 0$, if the word-context pair $(w,c)$ is unobserved, this will result in inf bieng computed. To remedy this, we could simply define a modified PMI that is equal to 0 when $n_{w,c} = 0$, which is the positive pointwise mutual information (PPMI) which:\n",
    "    P$$PPMI(w,c) = \\max(0, PMI(w,c))$$\n",
    "<br>\n",
    "This wonderfull explaination is made by <a href=\"http://czhai.cs.illinois.edu/\">Dr.ChengXiang (\"Cheng\") Zhai</a><br><br>\n",
    "\n",
    "<center><b>HINT: Consult your slides and see the example, how the formulas are used. You can calculate $|D|$ by the formula given in the slides(its the same thing).</b></center>\n",
    "\n",
    "If youre having troubles implementing this here is some [motivation](https://www.youtube.com/watch?v=TsyM5jP7RQk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a co-occurance matrix with +,- k window size\n",
    "Hint: Use the ngrams package from [nltk](https://www.nltk.org/) to make life easier. Matrix size is vocab X vocab.\n",
    "Please keep track of the order of words in the matrix this will be usefull later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating co-occurences\n"
     ]
    }
   ],
   "source": [
    "def com_single_tweet(tweet, vocab, k, mat):\n",
    "    all_grams = []\n",
    "    for i in range(2, k+2):\n",
    "        grams = list(ngrams(tweet, i))\n",
    "        grams = [tuple(sorted((x[0], x[-1]))) for x in grams] #I do not need middle elements of ngrams \\\n",
    "                                    #and (x, y) should be considered same as (y, x) for count in matrix\n",
    "        all_grams += grams\n",
    "    dist = nltk.FreqDist(all_grams)\n",
    "    for i in range(0, mat.shape[0]):\n",
    "        for j in range(0, mat.shape[1]):\n",
    "            mat[i, j] += dist[(vocab[i], vocab[j])]\n",
    "    return mat\n",
    "def coOccuranceMatrix(train_X,vocab,k=2):\n",
    "    \"\"\"\n",
    "    Return co-occurance matrix with ppmi counts\n",
    "    Args:\n",
    "        data : dataset\n",
    "        vocab : vocabulary\n",
    "    Returns: co_matrix\n",
    "    co_matrix: co-occurance matrix\n",
    "    \"\"\"\n",
    "    co_occurrence_matrix = np.zeros((len(vocab), len(vocab)))\n",
    "    count = 0\n",
    "    print(\"Calculating co-occurences\")\n",
    "    for tweet in train_X:\n",
    "        co_occurrence_matrix = com_single_tweet(tweet, vocab, k, co_occurrence_matrix)\n",
    "    return co_occurrence_matrix\n",
    "cooc_matrix = coOccuranceMatrix(X_train, vocabulary, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10846 1589379 1600225 1600225\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "cn = 0\n",
    "cc = 0\n",
    "w = 0\n",
    "for i in range(0, cooc_matrix.shape[0]):\n",
    "    for j in range(0, cooc_matrix.shape[1]):\n",
    "        if cooc_matrix[i, j] > 0:\n",
    "            cc += 1\n",
    "        else:\n",
    "            cn += 1\n",
    "        w += 1\n",
    "print(cc, cn, cn+cc, w)\n",
    "print(cooc_matrix)\n",
    "\n",
    "# from numpy import savetxt\n",
    "# savetxt('cooc_matrix.csv', cooc_matrix, delimiter=',')\n",
    "\n",
    "# from numpy import loadtxt\n",
    "# cooc_matrix = loadtxt('cooc_matrix.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1265, 1265)\n",
      "22519.0 22519.0\n"
     ]
    }
   ],
   "source": [
    "saved_matrix = cooc_matrix\n",
    "print(saved_matrix.shape)\n",
    "def ppmiMatrix(co_matrix):\n",
    "    \"\"\"\n",
    "    Return co-occurance matrix with ppmi counts\n",
    "\n",
    "    Args:\n",
    "        co_matrix : co-occurance matrix\n",
    "    Returns: ppmi_matrix\n",
    "    ppmi_co_matrix: co-occurance matrix with ppmi counts\n",
    "    \"\"\"\n",
    "    ppmi_co_matrix = np.zeros((co_matrix.shape[0], co_matrix.shape[1]))\n",
    "    ncs = np.sum(co_matrix, axis = 0) #sum the columns and out a row\n",
    "    nws = np.sum(co_matrix, axis = 1) # sum the rows and output a column\n",
    "    D = sum(ncs)\n",
    "    print(sum(ncs), sum(nws))\n",
    "    for i in range(0, co_matrix.shape[0]):\n",
    "        for j in range(0, co_matrix.shape[1]):\n",
    "            nwc = co_matrix[i, j]\n",
    "            if nwc != 0:\n",
    "                nc = ncs[j]\n",
    "                nw = nws[i]\n",
    "                tmp = min(0, np.log((nwc*D)/(nw*nc)))\n",
    "                ppmi_co_matrix[i, j] = tmp\n",
    "            else:\n",
    "                ppmi_co_matrix[i, j] = 0\n",
    "    return ppmi_co_matrix\n",
    "ppm = ppmiMatrix(saved_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for SVD has been provided for you, all you have to do is specify the number of top eigenvalues or how many top <b>n</b> dimensions you want to keep. Check the dimensions of the returned matrix by using <blockquote>.shape</blockquote> command to figure out if the embedding for each word is in row or column. By our calculation the vocab count should be less than five thousand, reduce the dimensionality to less than one thousand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1265, 1000)\n",
      "(1265, 1265)\n"
     ]
    }
   ],
   "source": [
    "#code provided \n",
    "def denseMatrixViaSVD(ppmi_co_matrix,n):\n",
    "    \"\"\"\n",
    "    Return reduced dimensionality co-occurance matrix by applying svd\n",
    "\n",
    "    Args:\n",
    "        ppmi_matrix : co-occurance matrix with ppmi counts\n",
    "        \n",
    "    Returns: svd_co_matrix\n",
    "    svd_co_matrix: reduced dimensionality co-occurance matrix\n",
    "    \"\"\"\n",
    "#     top_n_eigenvalues=\n",
    "    U, Sigma, VT = randomized_svd(ppmi_co_matrix, \n",
    "                              n_components=n,\n",
    "                              n_iter=5,\n",
    "                              random_state=None)\n",
    "    svd_co_matrix=U\n",
    "    return svd_co_matrix\n",
    "dmvs = denseMatrixViaSVD(ppm, 1000)\n",
    "print(dmvs.shape)\n",
    "print(ppm.shape)\n",
    "#each row is a word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling\n",
    "Now that we have our embeddings, lets use these to train a Feed Forward Neural network for our semantic classification task. Since a feed forward network's input layer is of a fixed size we will need to create a fixed size representation for each review. For this purpose we will use the following:\n",
    "<ul>\n",
    "    <li>Average pooling.</li>\n",
    "    <li>Averaging pooling algorithm by FastText(provided)</li>\n",
    "    <li>Max pooling. </li>\n",
    "</ul>\n",
    "For those of you who are familiar with Convolution Neural Networks this pooling will be a 1d pooling operation. See illustrated example below:<img src=\"pooling.png\">\n",
    "\n",
    "Since we cant have a tutorial due to corona virus for keras, a simple feed forward network has beed provided for you. You need to create train_X, test_X , train_Y and test_Y these should numpy arrays inorder for keras to use them.\n",
    "<ul>\n",
    "    <li>train_X= contains embedding representains of all the reviews in the train set</li>\n",
    "    <li>train_Y= contains embedding representains of all the reviews in the test set</li>\n",
    "    <li>train_Y= contains <b>one hot</b> representations of train labels</li>\n",
    "    <li>test_Y= contains <b>one hot</b> representations of test labels</li>   \n",
    "</ul>\n",
    "To construct one hot representation you can use the sklearn's preprocessing package or the preprocessing package from keras. Read online."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# CONSTRUCT ONE HOT REPRESENTATION\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "train_labels = enc.fit_transform([[x] for x in y_train]).toarray()\n",
    "test_labels = enc.transform([[x] for x in y_test]).toarray()\n",
    "# print(y_train[:20], train_labels[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_X_train(X, word_embs, vocab):\n",
    "    res = np.zeros((len(X), len(vocabulary)))\n",
    "    for i, tweet in enumerate(X):\n",
    "        for j, voc in enumerate(vocabulary):\n",
    "            res[i][j] = word_embs[j]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1265, 1000)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-bb4a634ec1e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfta_emb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mfta_X_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_X_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfta_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-58-1eeeccb553a8>\u001b[0m in \u001b[0;36mcreate_X_train\u001b[0;34m(X, word_embs, vocab)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtweet\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m             \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_embs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "#Fast text averaging, pass a list of word embeddings and embedding size to fasttextAveraging function\n",
    "def l2_norm(x):\n",
    "   return np.sqrt(np.sum(x**2))\n",
    "\n",
    "def div_norm(x):\n",
    "   norm_value = l2_norm(x)\n",
    "   if norm_value > 0:\n",
    "       return x * ( 1.0 / norm_value)\n",
    "   else:\n",
    "       return x\n",
    "def fasttextAveraging(embedding_list,embedding_size):\n",
    "    norm=np.zeros(embedding_size)\n",
    "    for emb in embedding_list:\n",
    "        norm=norm+div_norm(emb)\n",
    "    return norm/len(embedding_list)\n",
    "fta_emb = fasttextAveraging(dmvs, dmvs.shape)\n",
    "print(fta_emb.shape)\n",
    "\n",
    "fta_X_train = create_X_train(X_train, fta_emb, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1265, 1)\n"
     ]
    }
   ],
   "source": [
    "def averagePooling(embedding_list,embedding_size):\n",
    "    \"\"\"\n",
    "    Return average embedding vector from list of embedding\n",
    "    Args:\n",
    "        embedding_list : embedding list\n",
    "        embedding_size: size of embedding vector\n",
    "    Returns: average_embedding\n",
    "    average_embedding: average embedding vector\n",
    "    \"\"\"\n",
    "    res = np.zeros(embedding_size[0])\n",
    "    for i, emb in enumerate(embedding_list):\n",
    "        res[i] = np.sum(emb)/embedding_size[1]\n",
    "    res = res.reshape(-1, 1)\n",
    "    return res\n",
    "avg_emb = averagePooling(dmvs, dmvs.shape)\n",
    "print(avg_emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1265, 1) (691, 3)\n"
     ]
    }
   ],
   "source": [
    "def maxPooling(embedding_list,embedding_size):\n",
    "    \"\"\"\n",
    "    Return maxpooling embedding vector from list of embedding\n",
    "    Args:\n",
    "        embedding_list : embedding list\n",
    "        embedding_size: size of embedding vector\n",
    "    Returns: max_embedding\n",
    "    max_embedding: maxpooled embedding vector\n",
    "    \"\"\"\n",
    "    res = np.zeros(embedding_size[0])\n",
    "    for i, emb in enumerate(embedding_list):\n",
    "        res[i] = max(emb)\n",
    "    res = res.reshape(-1, 1)\n",
    "    return res\n",
    "max_emb = maxPooling(dmvs, dmvs.shape)\n",
    "print(max_emb.shape, train_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try using all three representaions to train the model and check which one works best. You can play around with embedding size by controlling <b>n</b> in SVD function and for the model you can add or remove layers or change the number of neurons in the hidden layers. Keep in mind that the layers should be decreasing in size as we go deeper into the network, theoritically this means that we are constructing complex features in a lower dimensional space from less complex features and larger dimensional space.<br><br>\n",
    "Issues related to overfiting will be proper addressed in the next assignment for now you are free to choose the number of epoch, try to find one that trains the model sufficiently enough but does not overfit it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 300)               300300    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 50)                15050     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 3)                 153       \n",
      "=================================================================\n",
      "Total params: 315,503\n",
      "Trainable params: 315,503\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "embedding_size=1000\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(embedding_size,)),#donot change(input layer)\n",
    "    keras.layers.Dense(300, activation='relu'),#(hidden layer)\n",
    "    keras.layers.Dense(50, activation='relu'),#(hidden layer)\n",
    "    keras.layers.Dense(3)#donot change\n",
    "])\n",
    "model.compile(optimizer='adam',\n",
    "              loss=[\"categorical_crossentropy\"],\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input arrays should have the same number of samples as target arrays. Found 1265 input samples and 691 target samples.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-2516e271c159>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m model.fit(fta_emb,train_labels, epochs=15, batch_size=32,\n\u001b[0;32m----> 2\u001b[0;31m                verbose=1,shuffle=True)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    630\u001b[0m         \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m         \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 632\u001b[0;31m         shuffle=shuffle)\n\u001b[0m\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2463\u001b[0m       \u001b[0;31m# Check that all arrays have the same length.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2464\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_distribution_strategy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2465\u001b[0;31m         \u001b[0mtraining_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_array_lengths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2466\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_graph_network\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2467\u001b[0m           \u001b[0;31m# Additional checks to avoid users mistakenly using improper loss fns.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mcheck_array_lengths\u001b[0;34m(inputs, targets, weights)\u001b[0m\n\u001b[1;32m    617\u001b[0m                      \u001b[0;34m'the same number of samples as target arrays. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    618\u001b[0m                      \u001b[0;34m'Found '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' input samples '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 619\u001b[0;31m                      'and ' + str(list(set_y)[0]) + ' target samples.')\n\u001b[0m\u001b[1;32m    620\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_w\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    621\u001b[0m     raise ValueError('All sample_weight arrays should have '\n",
      "\u001b[0;31mValueError\u001b[0m: Input arrays should have the same number of samples as target arrays. Found 1265 input samples and 691 target samples."
     ]
    }
   ],
   "source": [
    "model.fit(fta_emb,train_labels, epochs=15, batch_size=32,\n",
    "               verbose=1,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the <b>model.predict</b> method to get predictions. There predictions will be a probability distribution over the lables, to get the desired class take the max value in a prediction vector as the predicted class.<br> To run the code below you need to construct a list of unique labels, the list should be ordered on the basis of the id assigned to each class when you were constructing the one hot representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions = code here\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "test_Y_max=np.argmax(test_Y, axis=-1)\n",
    "cm=confusion_matrix(test_Y_max,predictions)\n",
    "cm1 = cm1.astype('float') / cm1.sum(axis=1)[:, np.newaxis]\n",
    "cm1 = pd.DataFrame(cm, labelList,labelList )# matrix,names row,names col,\n",
    "# plt.figure(figsize=(10,7))\n",
    "sn.set(font_scale=1.4) # for label size\n",
    "sn.heatmap(cm1, annot=True, annot_kws={\"size\": 11}, fmt=\".2f\") # font size\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification Report\\n\",classification_report(test_Y_max, predictions, labels=[0,1,2], target_names = labelList))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction base Embeddings\n",
    "For prediction based embeddings we will use the IMDB dataset. We will create create our embeddings by using the unlabeledTrainData.tsv file.\n",
    "We will use the Word2Vec model that we have already covered in class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "Since we have already discussed preprocessing trade off's in previous assingments. We expect you to analyse the data and preform the preprocessing that is required.<br> \n",
    "<b> Hint: Each review is in string format so they have used slahes to escape characters and br tags to identify line breaks</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(data):\n",
    "    \"\"\"\n",
    "    Return preprocessed data\n",
    "\n",
    "    Args:\n",
    "        data : reviews\n",
    "    \n",
    "    Returns: preprocessed_data\n",
    "    preprocessed_data : preprocessed dataset \n",
    "    \"\"\"\n",
    "#     preprocessed_data=\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the [gensim](https://radimrehurek.com/gensim/models/word2vec.html) to train a Word2Vec model. Keep the dimensionality at 300 and window size at 2. After trianing use the model and previously coded methods create vectorial represenations for movie reviews.<i>(create train_X, test_X, train_Y and test_Y)</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainWord2Vec(data):\n",
    "    \"\"\"\n",
    "    Return preprocessed data\n",
    "\n",
    "    Args:\n",
    "        data : movie reviews\n",
    "    \n",
    "    Returns: model\n",
    "    model : Word2Vec model \n",
    "    \"\"\"\n",
    "#     preprocessed_data=\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the train and test files and create the vectorial representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is dense representaion we wont be faced with the challenges posed by sparse representations. We can move onto modelling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "embedding_size=100\n",
    "model_word2vec = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(embedding_size,)),#donot change(input layer)\n",
    "    keras.layers.Dense(150, activation='relu'),#(hidden layer)\n",
    "    keras.layers.Dense(50, activation='relu'),#(hidden layer)\n",
    "    keras.layers.Dense(2)#donot change\n",
    "])\n",
    "model_word2vec.compile(optimizer='adam',\n",
    "              loss=[\"categorical_crossentropy\"],\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_word2vec.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_word2vec.fit(train_X,train_Y, epochs=15, batch_size=32,\n",
    "               verbose=1,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions = \n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "test_Y_max=np.argmax(test_Y, axis=-1)\n",
    "cm=confusion_matrix(test_Y_max,predictions)\n",
    "cm1 = cm1.astype('float') / cm1.sum(axis=1)[:, np.newaxis]\n",
    "cm1 = pd.DataFrame(cm, labelList,labelList )# matrix,names row,names col,\n",
    "# plt.figure(figsize=(10,7))\n",
    "sn.set(font_scale=1.4) # for label size\n",
    "sn.heatmap(cm1, annot=True, annot_kws={\"size\": 11}, fmt=\".2f\") # font size\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification Report\\n\",classification_report(test_Y_max, predictions, labels=[0,1], target_names = labelList))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theory\n",
    "The two are two major reaserch papers [Word2Vec](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) for prediction based embeddings and [GloVe](https://nlp.stanford.edu/pubs/glove.pdf) for frequency based embeddings. Research online and write a short note on the trade offs associated with the two types of embeddings. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###_______________Anwer________________###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ending Note:\n",
    "Feed forward networks are not suitable for natural language task because of thier fixed input sizes, the size of natural language text in each example for a dataset can vary considerably, also feed forward networks ignore the temporal nature of natural language text, which result's in them not bieng able to caputre context's or interdepencies between words for semantic information. To fix this researcher's have invented recurrent neural networks that help to aleviate these limitations.\n",
    "The next assignment will be related to recurrent neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We hope all of you are working on your projects!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
